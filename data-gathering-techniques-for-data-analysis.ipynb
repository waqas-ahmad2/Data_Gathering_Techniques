{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-01T20:35:26.486283Z","iopub.execute_input":"2023-06-01T20:35:26.486683Z","iopub.status.idle":"2023-06-01T20:35:26.493596Z","shell.execute_reply.started":"2023-06-01T20:35:26.486647Z","shell.execute_reply":"2023-06-01T20:35:26.492502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"##  **Elevate Your Analysis: Mastering Data Gathering Techniques**","metadata":{}},{"cell_type":"markdown","source":"This title highlights the importance of utilizing open source APIs and web scraping techniques to gather data for analysis when readily available data in CSV, txt or Excel files is limited. It conveys the idea that data analysis often requires exploring alternative sources to access valuable data for analysis purposes.\n\nThis notebook explores different methods and tools used to collect data from various sources and preparing it for analysis or for open source sharing. It covers techniques such as web scraping and API integration.","metadata":{}},{"cell_type":"markdown","source":"## **Table of Contents**\n1. Introduction\n2. Web Scraping\n3. API Integration\n4. Conclusion","metadata":{}},{"cell_type":"markdown","source":"## **1. Introduction**\nWelcome to the \"Data Gathering Techniques for Analysis\" notebook. In this notebook, we will explore the critical process of gathering data for analysis and its significance in deriving meaningful insights. We will highlight a range of techniques that enable us to acquire data from diverse sources, ensuring a comprehensive understanding of data collection methods.\n\n\n## **2. Web Scraping**\nThis section focuses on web scraping as a technique to extract data from websites. We will use BeautifulSoup library and other commonly used tools for web scraping. The code snippets below demonstrate how to scrape data from HTML web pages and store it in a structured format for analysis.\n\nWe will focus on web scraping techniques to gather user reviews data from the Consumer Affairs [website](http://www.consumeraffairs.com), specifically targeting reviews related to Pizza Hut.It is worth noting that the same techniques can be applied (with some tweaking) to scrape reviews for other businesses or products as well, providing a versatile approach to data gathering.\n\nConsumer Affairs serves as an invaluable platform for consumers to share their feedback and opinions, making it an excellent source of information for analysis purposes.\n\nThis data can serve as a valuable resource for sentiment analysis, trend identification, and customer satisfaction assessments.\n\nLet's proceed with web scraping techniques to gather the desired Pizza Hut reviews data\n","metadata":{}},{"cell_type":"code","source":"# importing neccesary libraries\nimport requests\nimport pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup as bs\n\n# Set the number of pages to scrape\npages = 6\n\n# Create an empty DataFrame to store the final results\nfinal_df = pd.DataFrame()\n\n# Iterate through each page\nfor page in range(1, pages+1):\n\n    # Set the request header\n    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'}\n\n    # Construct the URL for the current page\n    url = f'https://www.consumeraffairs.com/food/pizza-hut.html?page={page}'\n\n    # Send a GET request to the URL\n    response = requests.get(url, headers=header)\n    \n    # Create a BeautifulSoup object to parse the HTML content\n    soup = bs(response.text)\n    \n    # Beautify the HTML content (optional)\n    soup.prettify()\n    \n    # Find all the relevant div elements for name, location, review date, review, and star rating\n    first = soup.find_all('div', attrs={'class': \"rvw-bd\"})\n    second = soup.find_all('div', attrs={'class': \"rvw-aut\"})\n    third = soup.find_all('div', attrs={'class': \"rvw__hdr\"})\n\n    # Initialize empty lists to store the extracted data\n    name = []\n    location = []\n    review_date = []\n    review = []\n    star_rating = []\n    \n    # Extract the review date for each review\n    for i in first:\n        try:\n            review_date.append(i.find('span').text.split(': ')[1])\n        except:\n            review_date.append(np.nan)\n    \n    # Extract the review text for each review\n    for i in first:\n        try:\n            review.append(i.find_all('p')[1].text)\n        except:\n            review.append('no review')\n    \n    # Extract the name and location for each review\n    for i in second:\n        try:\n            temp_lst = i.find('span').text.split(' of ')\n            name.append(temp_lst[0])\n            location.append(temp_lst[1])\n        except:\n            name.append('unknown')\n            location.append('unknown')\n    \n    # Extract the star rating for each review\n    for i in third:\n        try:\n            # Find the meta tag with itemprop=\"ratingValue\" and extract the content attribute value\n            rating = i.find('meta', attrs={'itemprop': 'ratingValue'}).get('content')\n            if rating:\n                star_rating.append(rating)\n            else:\n                star_rating.append(np.nan)\n        except:\n            star_rating.append(np.nan)\n    \n    # Create a temporary DataFrame with the extracted data for the current page\n    temp_df = pd.DataFrame({'name': name, 'location': location, 'review_date': review_date, 'review': review, 'star_rating': star_rating})\n    \n    # Concatenate the temporary DataFrame with the final DataFrame\n    final_df = pd.concat([final_df, temp_df], ignore_index=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation of the main points of the code:**\n\n1. The code uses web scraping techniques to gather user reviews data from the Consumer Affairs website, focusing on Pizza Hut reviews.\n\n2. The requests library is used to send HTTP requests, and the BeautifulSoup library is used to parse the HTML content of the website.\n3. The code specifies the number of pages to scrape and creates an empty DataFrame (final_df) to store the results.\n4. It loops through each page, sends a GET request to the website, and parses the HTML content using BeautifulSoup.\n5. The code extracts the relevant information from the HTML, including the name, location, review date, review text, and star rating of each review.\n6. Exception handling is implemented to handle cases where the desired information is not available or encounters an error.\n7. The extracted data is stored in temporary lists, and a temporary DataFrame (temp_df) is created for each page.\n8. The temporary DataFrame is concatenated with the final DataFrame (final_df) to combine the data from all pages.\n9. After the loop finishes, the final DataFrame contains the scraped data from all pages.\n10. The scraped data can be further analyzed, processed, or saved for future use.","metadata":{}},{"cell_type":"code","source":"final_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-02T08:22:40.398363Z","iopub.execute_input":"2023-06-02T08:22:40.400039Z","iopub.status.idle":"2023-06-02T08:22:40.429535Z","shell.execute_reply.started":"2023-06-02T08:22:40.399985Z","shell.execute_reply":"2023-06-02T08:22:40.428133Z"},"trusted":true},"execution_count":204,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 713 entries, 0 to 712\nData columns (total 5 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   name         713 non-null    object\n 1   location     713 non-null    object\n 2   review_date  713 non-null    object\n 3   review       713 non-null    object\n 4   star_rating  681 non-null    object\ndtypes: object(5)\nmemory usage: 28.0+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":" ## **3. API Integration**\n\nIn this section, we delve into the process of integrating APIs to gather data for analysis. Our focus is on utilizing the TMDB Top Rated API, which provides information about the highest-rated movies. By leveraging this API, we can access a comprehensive dataset on popular films.\n\nTo demonstrate the API integration, we utilize the [RapidAPI](https://rapidapi.com/collection/list-of-free-apis) platform, which offers a range of open-source APIs. The provided code snippets showcase how to establish a connection with the TMDB API, send requests to retrieve the desired data, and handle the received data for analysis.\n\nAuthentication is an essential aspect when working with APIs, and we cover this topic in the context of accessing the TMDB API. The code examples highlight the necessary steps to authenticate your requests, ensuring seamless access to the desired data.\n\nBy leveraging APIs, we can tap into extensive sources of information and harness it for our data analysis endeavors. The provided code snippets serve as practical illustrations, guiding the process of fetching data from the TMDB Top Rated API for analysis purposes.","metadata":{}},{"cell_type":"code","source":"# importing libraries\nimport requests\nimport pandas as pd\nimport numpy as np\n\n# Set the number of pages to fetch\npages = 500\n\n# Create an empty DataFrame\nfinal_df = pd.DataFrame()\n\n# Loop through each page\nfor page in range(1, pages+1):\n    # Construct the URL for the API request\n    url = \"https://api.themoviedb.org/3/movie/top_rated?language=en-US&page={}\".format(page)\n\n    # Set the headers for the API request\n    headers = {\n        \"accept\": \"application/json\",\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n    }\n\n    # Send the API request\n    response = requests.get(url, headers=headers)\n\n    # Convert the response to a DataFrame\n    temp_df = pd.DataFrame(response.json()['results'])\n    temp_df = temp_df[['id', 'title', 'release_date', 'overview', 'popularity', 'vote_average', 'vote_count']]\n\n    # Append the data to the final DataFrame\n    final_df = pd.concat([final_df, temp_df], ignore_index=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation of the main points of the code:**\n\n1. The code above fetches data from the TMDB API for the top-rated movies. It retrieves data from multiple pages by iterating over the specified number of pages (500 in this case).\n\n2. To use this code, you'll need to replace 'YOUR_API_KEY' in the Authorization header with your actual TMDB API key.\n\n3. The API response is converted to a DataFrame, and only the desired columns (id, title, release_date, overview, popularity, vote_average, vote_count) are selected.\n\n4. The fetched data is then appended to the final_df DataFrame using pd.concat() with the ignore_index=True parameter to ensure proper indexing.\n","metadata":{}},{"cell_type":"code","source":"final_df","metadata":{"execution":{"iopub.status.busy":"2023-06-02T05:40:50.219978Z","iopub.execute_input":"2023-06-02T05:40:50.220925Z","iopub.status.idle":"2023-06-02T05:40:50.244268Z","shell.execute_reply.started":"2023-06-02T05:40:50.220879Z","shell.execute_reply":"2023-06-02T05:40:50.242999Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"          id                        title release_date  \\\n0        238                The Godfather   1972-03-14   \n1        278     The Shawshank Redemption   1994-09-23   \n2        240        The Godfather Part II   1974-12-20   \n3      19404  Dilwale Dulhania Le Jayenge   1995-10-19   \n4        424             Schindler's List   1993-12-15   \n...      ...                          ...          ...   \n9995   12521                      Shocker   1989-10-27   \n9996  298722                   Soap Opera   2014-10-23   \n9997  467673                     Budapest   2018-06-27   \n9998  101179                Truth or Dare   2012-08-05   \n9999    2900         The Astronaut's Wife   1999-08-26   \n\n                                               overview  popularity  \\\n0     Spanning the years 1945 to 1955, a chronicle o...      97.119   \n1     Framed in the 1940s for the double murder of h...      82.607   \n2     In the continuing saga of the Corleone crime f...      63.884   \n3     Raj is a rich, carefree, happy-go-lucky second...      22.713   \n4     The true story of how businessman Oskar Schind...      46.255   \n...                                                 ...         ...   \n9995  After being sent to the electric chair, a seri...      10.974   \n9996  When the kooky tenants of an apartment block e...       3.860   \n9997  Two best friends stuck in boring jobs become b...       5.854   \n9998  A group of friends are lured to an isolated ca...      11.796   \n9999  When astronaut Spencer Armacost returns to Ear...      14.331   \n\n      vote_average  vote_count  \n0              8.7       18014  \n1              8.7       23861  \n2              8.6       10879  \n3              8.6        4137  \n4              8.6       14114  \n...            ...         ...  \n9995           5.6         353  \n9996           5.6         227  \n9997           5.6         389  \n9998           5.6         312  \n9999           5.6         813  \n\n[10000 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>release_date</th>\n      <th>overview</th>\n      <th>popularity</th>\n      <th>vote_average</th>\n      <th>vote_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>238</td>\n      <td>The Godfather</td>\n      <td>1972-03-14</td>\n      <td>Spanning the years 1945 to 1955, a chronicle o...</td>\n      <td>97.119</td>\n      <td>8.7</td>\n      <td>18014</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>278</td>\n      <td>The Shawshank Redemption</td>\n      <td>1994-09-23</td>\n      <td>Framed in the 1940s for the double murder of h...</td>\n      <td>82.607</td>\n      <td>8.7</td>\n      <td>23861</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>240</td>\n      <td>The Godfather Part II</td>\n      <td>1974-12-20</td>\n      <td>In the continuing saga of the Corleone crime f...</td>\n      <td>63.884</td>\n      <td>8.6</td>\n      <td>10879</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19404</td>\n      <td>Dilwale Dulhania Le Jayenge</td>\n      <td>1995-10-19</td>\n      <td>Raj is a rich, carefree, happy-go-lucky second...</td>\n      <td>22.713</td>\n      <td>8.6</td>\n      <td>4137</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>424</td>\n      <td>Schindler's List</td>\n      <td>1993-12-15</td>\n      <td>The true story of how businessman Oskar Schind...</td>\n      <td>46.255</td>\n      <td>8.6</td>\n      <td>14114</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>12521</td>\n      <td>Shocker</td>\n      <td>1989-10-27</td>\n      <td>After being sent to the electric chair, a seri...</td>\n      <td>10.974</td>\n      <td>5.6</td>\n      <td>353</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>298722</td>\n      <td>Soap Opera</td>\n      <td>2014-10-23</td>\n      <td>When the kooky tenants of an apartment block e...</td>\n      <td>3.860</td>\n      <td>5.6</td>\n      <td>227</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>467673</td>\n      <td>Budapest</td>\n      <td>2018-06-27</td>\n      <td>Two best friends stuck in boring jobs become b...</td>\n      <td>5.854</td>\n      <td>5.6</td>\n      <td>389</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>101179</td>\n      <td>Truth or Dare</td>\n      <td>2012-08-05</td>\n      <td>A group of friends are lured to an isolated ca...</td>\n      <td>11.796</td>\n      <td>5.6</td>\n      <td>312</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>2900</td>\n      <td>The Astronaut's Wife</td>\n      <td>1999-08-26</td>\n      <td>When astronaut Spencer Armacost returns to Ear...</td>\n      <td>14.331</td>\n      <td>5.6</td>\n      <td>813</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **4. Conclusion**\n\nIn the final section, we summarize the key takeaways from this notebook. We emphasize the importance of data gathering techniques in the data analysis process and discuss the advantages and considerations associated with each method. We also suggest potential future directions for expanding on the techniques covered.","metadata":{}}]}
